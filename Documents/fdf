Please validate my understanding based on the endpoint information shared by you.

As per my understanding, there are a number of jobs running on the system.
The '/list_jobs' endpoint gives us a list of all running, pending, and completed jobs and their configuration_ids.
The airflow dag must filter out the pending configuration ids and run them.

Based on the above scenario, the dag designed by me will hit the 'list_jobs' API on a schedule (suppose every 10 minutes) and then start the workflow for all the pending configuration_ids.
Please validate if the above use case is correct.

Also, please give us details on the structure of the response given by the GET API endpoint.
Provide us with as much detail as you can on the following points.
The Structure of the Response by all API Endpoints.
Where are the APIs hosted and if there are multiple servers that we need to hit
What should happen if APIs stop responding or down.
Who should be notified if the APIs are not responding?
What is the usual workload of the jobs on the system
How much workload the server can take as in how many configuration ids can be processed in parallel
Please provide as much detail as you can so that we can capture the correct use case and design our airflow dag with accuracy.
